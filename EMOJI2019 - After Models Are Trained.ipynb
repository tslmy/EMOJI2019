{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, emoji\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE #!conda install -c conda-forge multicore-tsne -y\n",
    "from matplotlib import pyplot as plt\n",
    "import _lookup_tables\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "np = pd.np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "all_emjs = pd.Series(emoji.UNICODE_EMOJI).index.to_series()\n",
    "nrc_df = pd.read_csv('nrc_selected.csv') # Load NRC lexicon: Has columns 'zh', 'en', 'Sad', etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = gensim.models.FastText\n",
    "corpus_name = 'Weibo'\n",
    "num_epochs = 5\n",
    "liwc_fpath = 'liwc_cn.csv'\n",
    "nrc_df = nrc_df.drop_duplicates('zh').drop(['en'], axis=1).set_index('zh').astype(bool)\n",
    "\n",
    "Model = gensim.models.FastText\n",
    "corpus_name = 'TwtUs'\n",
    "num_epochs = 5\n",
    "liwc_fpath = 'liwc_en.csv'\n",
    "nrc_df = nrc_df.drop_duplicates('en').drop(['zh'], axis=1).set_index('en').astype(bool)\n",
    "\n",
    "# --------\n",
    "model_fpath  = f'gensimModels/{corpus_name}2014{Model.__name__}/0'\n",
    "model        = Model.load(model_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make t-SNE Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compress 100D Emoji Vectors To 2D Coordinates\n",
    "\n",
    "IF_WORK_ON_NRC      = True\n",
    "IF_WORK_ON_EMJ      = True\n",
    "\n",
    "def getTokenCnt(token):\n",
    "    try: return model.wv.vocab.get(token).count\n",
    "    except AttributeError: return\n",
    "    \n",
    "if IF_WORK_ON_EMJ: emj_cnts = dict()\n",
    "\n",
    "for epoch in range(num_epochs): #with tqdm(range(num_epochs), desc='Epochs') as epochs:\n",
    "    print(f'Loading Model {corpus_name}2014{Model.__name__}/{epoch}...')\n",
    "    model   = Model.load(f'gensimModels/{corpus_name}2014{Model.__name__}/{epoch}')\n",
    "    new_index = pd.Series({k: v.index for k, v in model.wv.vocab.items()})\n",
    "    vecs_df = pd.DataFrame(model.wv.vectors)\n",
    "    vecs_df = vecs_df.reindex(new_index)\n",
    "    vecs_df.index = new_index.index\n",
    "    print(f'There are {len(vecs_df)} tokens captured in this corpora.')\n",
    "    if IF_WORK_ON_NRC:\n",
    "        print('Querying model for all tokens in NRC...')\n",
    "        # Make nrc categorical vectors:\n",
    "        def _(tkn):\n",
    "            try: return model.wv.word_vec(tkn)\n",
    "            except KeyError: return None\n",
    "        nrc_tkns_vecs_df = nrc_df.index.to_series().apply(_).dropna().apply(pd.Series)\n",
    "        nrc_ctgy_vecs_df = nrc_df.apply(lambda c: nrc_tkns_vecs_df.reindex(c[c].index).mean()).T\n",
    "        print('Saving nrc-related vector lookup tables...')\n",
    "        nrc_tkns_vecs_df.to_hdf('data.hdf', f'{corpus_name}_{Model.__name__}_epoch{epoch}_nrc_tkns_vecs_df')\n",
    "        nrc_ctgy_vecs_df.to_hdf('data.hdf', f'{corpus_name}_{Model.__name__}_epoch{epoch}_nrc_ctgy_vecs_df')\n",
    "    if IF_WORK_ON_EMJ:\n",
    "        emj_vecs_df= vecs_df.reindex(emoji.UNICODE_EMOJI).dropna() #getVectorsFromModel(model, tokens=emoji.UNICODE_EMOJI)  # Emoji vectors and coordinates:\n",
    "        emj_vecs_df.to_hdf('data.hdf', f'{corpus_name}_{Model.__name__}_epoch{epoch}_emj_vecs_df') # Save:\n",
    "        emj_cnts[epoch] = all_emjs.apply(getTokenCnt).dropna().astype(int)\n",
    "if IF_WORK_ON_EMJ:\n",
    "    emj_cnts_df = pd.DataFrame(emj_cnts).T\n",
    "    emj_cnts_df.to_hdf('data.hdf', f'{corpus_name}_{Model.__name__}_emj_cnts_df')\n",
    "\n",
    "## Plot t-SNE maps\n",
    "\n",
    "corpora = {'TwtUs', 'Weibo'}\n",
    "emjs = {corpus_name: set(pd.read_hdf('data.hdf', f'{corpus_name}_{Model.__name__}_emj_cnts_df').columns) for corpus_name in corpora}\n",
    "common_emjs = set.intersection(*emjs.values())\n",
    "\n",
    "emjs_sorted_by_occurence = pd.concat([pd.read_hdf('data.hdf', f'TwtUs_{Model.__name__}_emj_cnts_df'),\n",
    "    pd.read_hdf('data.hdf', f'Weibo_{Model.__name__}_emj_cnts_df')], sort=True).dropna(axis=1).sum().sort_values(ascending=False).index.to_series()\n",
    "\n",
    "def plotEmojis(emj_coords_df, use_approx=False):\n",
    "    fig, ax = plt.subplots(figsize=(50, 50))\n",
    "    emjs_inModel = emj_coords_df.index.to_series()\n",
    "    emjs_inModel_asUnicode = _lookup_tables.emj2unicode[emjs_inModel].str.replace('_', '-').str.lstrip('0')\n",
    "    assert emjs_inModel_asUnicode.notna().all()\n",
    "    assert emjs_inModel_asUnicode.nunique()==len(emjs_inModel_asUnicode)==emjs_inModel.nunique()==len(emjs_inModel)\n",
    "    emjs_inModel_asImg = _lookup_tables.unicode2img[emjs_inModel_asUnicode]\n",
    "    emjs_inModel_asImg.index = emjs_inModel\n",
    "    if use_approx:\n",
    "        unlinked = pd.Series(emjs_inModel_asImg[emjs_inModel_asImg.isna()].index)\n",
    "        unlinked_rematched = unicode2img.reindex(unlinked.apply(lambda s: s[:s.rfind('-')] if s.rfind('-')>-1 else None))\n",
    "        unlinked_rematched.index = unlinked\n",
    "        unlinked_rematched.dropna(inplace=True)\n",
    "        emjs_inModel_asImg[unlinked_rematched.index] = unlinked_rematched\n",
    "    for i, row in emj_coords_df.assign(img=emjs_inModel_asImg).dropna().iterrows():\n",
    "        ax.add_artist(AnnotationBbox(OffsetImage(row.img), (row.x, row.y), xycoords='data', boxcoords=\"offset points\", frameon=False))\n",
    "    ax.update_datalim(emj_coords_df.values)\n",
    "    ax.autoscale()\n",
    "    return ax\n",
    "\n",
    "def compressWordVectors(emj_vecs_df, perplexity=30):\n",
    "    print('Compressing word vectors to coordinates in 2D via t-SNE...')\n",
    "    emj_coords = TSNE(n_jobs=24, random_state=42, n_iter=10000, perplexity=perplexity).fit_transform(emj_vecs_df.values) # reduce dimension with t-SNE\n",
    "    emj_coords_df = pd.DataFrame(emj_coords, index=emj_vecs_df.index).rename(columns={0: 'x', 1: 'y'}) # add index\n",
    "    return emj_coords_df\n",
    "\n",
    "model         = Model.load(f'gensimModels/{corpus_name}2014{Model.__name__}/0') # Load model. We only have to load one epoch.\n",
    "emj_vecs_df   = pd.read_hdf('data.hdf', f'{corpus_name}_{Model.__name__}_epoch0_emj_vecs_df').reindex(common_emjs)\n",
    "emj_coords_df = compressWordVectors(emj_vecs_df, perplexity=15)        # Get coordinates\n",
    "emj_coords_df.to_hdf('data.hdf', f'{corpus_name}_{Model.__name__}_emj_coords_df')\n",
    "# Plot:\n",
    "print('Plotting...')\n",
    "ax = plotEmojis(emj_coords_df)\n",
    "ax.set_title(f'{corpus_name} {Model.__name__} t-SNE Map')\n",
    "plt.savefig(f'tSNEs/{corpus_name}_{Model.__name__}_tSNE.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Emoji-to-Emotion Similarities (i.e., Vectorial Projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Customizable parameters:\n",
    "\n",
    "IF_REMOVE_AVG_PROJ   = False\n",
    "IF_SAVE_TO_HDF       = True\n",
    "IF_AGG_ACROSS_EPOCHS = True\n",
    "\n",
    "## Utilities: \n",
    "\n",
    "from sklearn import preprocessing # l2-normalize the samples (rows). \n",
    "getCommonItems = lambda a, b: list(set(a).intersection(b))\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def project(tkns_vecs_df, axis_vecs_df, remove_common_components=True, remove_common_components_by_group=False, if_normalize_axis=True, if_normalize_tkns=True):\n",
    "    assert len(tkns_vecs_df.columns)==len(axis_vecs_df.columns), 'Tokens and Axes must be in identical dimentionality.'\n",
    "    if remove_common_components: # Optionally remove common components from all vectors being considered:\n",
    "        common_components = pd.concat([tkns_vecs_df, axis_vecs_df], axis=0).mean()\n",
    "        tkns_vecs_df = tkns_vecs_df-common_components\n",
    "        axis_vecs_df = axis_vecs_df-common_components\n",
    "    if remove_common_components_by_group: # Optionally remove common components from all vectors being considered:\n",
    "        tkns_vecs_df = tkns_vecs_df-tkns_vecs_df.mean()\n",
    "        axis_vecs_df = axis_vecs_df-axis_vecs_df.mean()\n",
    "    else:\n",
    "        if if_normalize_axis: axis_vecs_df = pd.DataFrame(preprocessing.normalize(axis_vecs_df, norm='l2'), index=axis_vecs_df.index)\n",
    "        if if_normalize_tkns: tkns_vecs_df = pd.DataFrame(preprocessing.normalize(tkns_vecs_df, norm='l2'), index=tkns_vecs_df.index)\n",
    "    # Calculate projection via cosine similarity:\n",
    "    return pd.DataFrame(cosine_similarity(tkns_vecs_df, axis_vecs_df), columns=axis_vecs_df.index, index=tkns_vecs_df.index)\n",
    "\n",
    "## Actual work:\n",
    "\n",
    "for corpus_name in corpora:\n",
    "    if IF_AGG_ACROSS_EPOCHS: emj2nrcCtgy_proj_dfs = []\n",
    "    with tqdm(range(num_epochs), desc=f'Epochs in {corpus_name} {Model.__name__}') as epochs:\n",
    "        for epoch in epochs:\n",
    "            emj_vecs_df       = pd.read_hdf('data.hdf', f'{corpus_name}_{Model.__name__}_epoch{epoch}_emj_vecs_df')\n",
    "            if IF_REMOVE_AVG_PROJ: emj_vecs_df.loc['avg'] = emj_vecs_df.mean()\n",
    "            nrc_ctgy_vecs_df  = pd.read_hdf('data.hdf', f'{corpus_name}_{Model.__name__}_epoch{epoch}_nrc_ctgy_vecs_df')\n",
    "            emj2nrcCtgy_proj_df = project(tkns_vecs_df=emj_vecs_df, axis_vecs_df=nrc_ctgy_vecs_df, remove_common_components=False, remove_common_components_by_group=True)\n",
    "            if IF_REMOVE_AVG_PROJ:\n",
    "                emj2nrcCtgy_proj_df -= emj2nrcCtgy_proj_df.loc['avg']\n",
    "                emj2nrcCtgy_proj_df.drop('avg', inplace=True)\n",
    "            if IF_SAVE_TO_HDF: emj2nrcCtgy_proj_df.to_hdf('data.hdf', f'{corpus_name}_{Model.__name__}_epoch{epoch}_emj2nrcCtgy_proj_df')\n",
    "            if IF_AGG_ACROSS_EPOCHS: emj2nrcCtgy_proj_dfs.append(emj2nrcCtgy_proj_df)\n",
    "    if IF_AGG_ACROSS_EPOCHS:\n",
    "        g = pd.concat(emj2nrcCtgy_proj_dfs).reset_index().rename(columns={'index': 'emj'}).groupby('emj')\n",
    "        emj2nrcCtgy_proj_avg_df = g.mean()\n",
    "        emj2nrcCtgy_proj_std_df = g.std()\n",
    "        # Filter for stable emojis only: .stack()[_<_.describe()['75%']].unstack(1)\n",
    "        #_ = emj2nrcCtgy_proj_std_df.stack()\n",
    "        if IF_SAVE_TO_HDF: # Save to HDF:\n",
    "            emj2nrcCtgy_proj_avg_df.to_hdf('data.hdf', f'{corpus_name}_{Model.__name__}_emj2nrcCtgy_proj_avg_df')\n",
    "            emj2nrcCtgy_proj_std_df.to_hdf('data.hdf', f'{corpus_name}_{Model.__name__}_emj2nrcCtgy_proj_std_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Radar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare utilities:\n",
    "import json\n",
    "html_template = open('RadarChartsTemplate.html', 'r').read()\n",
    "emj2ucCtgy = _lookup_tables.emj_ucCtgy.str.replace(r'\\W', '')  # prepare Unicode Category look-up table\n",
    "row2dict = lambda row: list(row.rename('value').reset_index().rename(columns={'index': 'axis'}).T.to_dict().values())\n",
    "emj2desc = pd.Series(emoji.UNICODE_EMOJI).str[1:-1].str.replace('_', ' ').str.title()\n",
    "\n",
    "# prepare numerical data:\n",
    "def fixRangeForRadarPlot(proj_df, IF_FILL_TO_ONE = True, IF_STANDARIZE = False):\n",
    "    if IF_STANDARIZE: \n",
    "        standardize = lambda s: (s-s.min())/(s.max()-s.min())\n",
    "        proj_df = proj_df.apply(standardize)\n",
    "    if IF_FILL_TO_ONE: proj_df *= 1/proj_df.abs().max().max() # At this stage, although theoretical range of the consine similarities is -1~+1, we would like to zoom into (while keeping 0 at 0) the values so that the max is 1 OR the min is -1:\n",
    "    proj_df += 1 # move the range from -1~+1 to 0~2.\n",
    "    proj_df /= 2 # compress the range from 0~2 to 0~1.\n",
    "    return proj_df*100\n",
    "weibo_projs_df = pd.read_hdf('data.hdf', f'Weibo_{Model.__name__}_emj2nrcCtgy_proj_avg_df').reindex(common_emjs)\n",
    "twitr_projs_df = pd.read_hdf('data.hdf', f'TwtUs_{Model.__name__}_emj2nrcCtgy_proj_avg_df').reindex(common_emjs)\n",
    "weibo_projs_df = fixRangeForRadarPlot(weibo_projs_df)\n",
    "twitr_projs_df = fixRangeForRadarPlot(twitr_projs_df)\n",
    "\n",
    "# Prepare list of Emojis to plot:\n",
    "order = 'dissimilarity'#'occurence'\n",
    "takeTop = lambda df: df.head(5).index.to_series().reset_index(drop=True)\n",
    "if order=='dissimilarity':\n",
    "    _ = pd.DataFrame({'diff': (weibo_projs_df - twitr_projs_df).abs().mean(axis=1),\n",
    "                      'ctgy': _lookup_tables.emj_ucCtgy})\n",
    "    _.dropna(inplace=True)\n",
    "    _.sort_values('diff', ascending=False, inplace=True)\n",
    "    _ = _.groupby('ctgy').apply(takeTop).drop(['Flags', 'Activities', 'Symbols'])\n",
    "elif order=='occurence':\n",
    "    _ = emjs_sorted_by_occurence.to_frame().assign(ctgy=_lookup_tables.emj_ucCtgy)\n",
    "    _ = _.groupby('ctgy').apply(takeTop).drop(['Flags', 'Activities', 'Symbols'])\n",
    "emj_to_plot = _.tolist()\n",
    "\n",
    "# paint each:\n",
    "data_to_plot = []\n",
    "for token in emj_to_plot:\n",
    "    weibo_row, twitr_row = weibo_projs_df.loc[token], twitr_projs_df.loc[token]\n",
    "    weibo_row.index = twitr_row.index\n",
    "    data_to_plot.append([\n",
    "        emj2ucCtgy[token], # The Unicode Category that this emoji belongs to\n",
    "        emj2desc[token]  , # The Unicode description of this emoji\n",
    "        token,  # This very Emoji itself\n",
    "        {'className': 'Twitter', 'axes': row2dict(twitr_row)},\n",
    "        {'className': 'Weibo'  , 'axes': row2dict(weibo_row)}])\n",
    "    \n",
    "ctgys_used             = emj2ucCtgy[emj_to_plot].drop_duplicates()\n",
    "ctgys_used_unsanitized = _lookup_tables.emj_ucCtgy[emj_to_plot].drop_duplicates()\n",
    "open('RadarCharts.html', 'w').write(html_template\\\n",
    "                               .replace('%%%%%%', json.dumps(data_to_plot, ensure_ascii=False))\\\n",
    "                               .replace('^^^^^^', ''.join('<div class=\"row\" id=\"'+ctgys_used+'\"><h1>'+ctgys_used_unsanitized+'</h1></div>'))\n",
    "                               #.replace('&&&&&&', json.dumps(list(twitr_projs_df.index), ensure_ascii=False)))\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'FastText'\n",
    "corpora = {'TwtUs', 'Weibo'}\n",
    "\n",
    "emjs = {corpus_name: set(pd.read_hdf('data.hdf', f'{corpus_name}_{model_name}_emj_cnts_df').columns) for corpus_name in corpora}\n",
    "common_emjs = set.intersection(*emjs.values())\n",
    "\n",
    "weibo_projs_df = pd.read_hdf('data.hdf', f'Weibo_{model_name}_emj2nrcCtgy_proj_avg_df').reindex(common_emjs)\n",
    "twitr_projs_df = pd.read_hdf('data.hdf', f'TwtUs_{model_name}_emj2nrcCtgy_proj_avg_df').reindex(common_emjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_projs_corr_df = weibo_projs_df.corr('spearman')\n",
    "twitr_projs_corr_df = twitr_projs_df.corr('spearman')\n",
    "# Sort the emotions by average correlation:\n",
    "avg_projs_corr_df = (weibo_projs_corr_df+twitr_projs_corr_df)/2\n",
    "emo_order = avg_projs_corr_df.mean().sort_values(ascending=False).index\n",
    "weibo_projs_corr_df = weibo_projs_corr_df.loc[emo_order, emo_order]\n",
    "twitr_projs_corr_df = twitr_projs_corr_df.loc[emo_order, emo_order]\n",
    "avg_projs_corr_df   = avg_projs_corr_df  .loc[emo_order, emo_order]\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def computeAvgEmoSimiDf(corpus_name):\n",
    "    simi_dfs = []\n",
    "    for epoch in range(5):\n",
    "        emos_df = pd.read_hdf('data.hdf', f'{corpus_name}_{model_name}_epoch{epoch}_nrc_ctgy_vecs_df')\n",
    "        simi_df = pd.DataFrame(cosine_similarity(emos_df-emos_df.mean()), index=emos_df.index, columns=emos_df.index).stack()\n",
    "        simi_dfs.append(simi_df)\n",
    "    return pd.concat(simi_dfs, axis=1).mean(axis=1).unstack()\n",
    "weibo_emo_simi_df = computeAvgEmoSimiDf('Weibo')\n",
    "twitr_emo_simi_df = computeAvgEmoSimiDf('TwtUs')\n",
    "\n",
    "# Sort the emotions by average correlation:\n",
    "avg_emo_simi_df = (weibo_emo_simi_df+twitr_emo_simi_df)/2\n",
    "#emo_order = avg_emo_simi_df.mean().sort_values(ascending=False).index\n",
    "weibo_emo_simi_df = weibo_emo_simi_df.loc[emo_order, emo_order]\n",
    "twitr_emo_simi_df = twitr_emo_simi_df.loc[emo_order, emo_order]\n",
    "avg_emo_simi_df   = avg_emo_simi_df  .loc[emo_order, emo_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax11, ax12), (ax21, ax22)) = plt.subplots(2,2, figsize=(11, 8))\n",
    "sns.heatmap(weibo_projs_corr_df, annot=True, ax=ax11, vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "ax11.set_title('Pairwise Correlation in Weibo (China)')\n",
    "ax11.set_xticks([])\n",
    "sns.heatmap(twitr_projs_corr_df, annot=True, ax=ax12, vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "ax11.set_xlabel('(a)')\n",
    "\n",
    "ax12.set_title('Pairwise Correlation in Twitter (US)')\n",
    "ax12.set_xticks([])\n",
    "ax12.set_yticks([])\n",
    "sns.heatmap(  avg_projs_corr_df, annot=True, ax=ax13, vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "ax12.set_xlabel('(b)')\n",
    "\n",
    "sns.heatmap(weibo_emo_simi_df.round(2), annot=True, ax=ax21, vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "ax21.set_title('Pairwise Similarity in Weibo (China)')\n",
    "ax21.set_xticklabels(ax21.get_xticklabels(), rotation=45)\n",
    "ax21.set_xlabel('(c)')\n",
    "\n",
    "sns.heatmap(twitr_emo_simi_df.round(2), annot=True, ax=ax22, vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "ax22.set_title('Pairwise Similarity in Twitter (US)')\n",
    "ax22.set_xticklabels(ax22.get_xticklabels(), rotation=45)\n",
    "ax22.set_yticks([])\n",
    "ax22.set_xlabel('(d)')\n",
    "\n",
    "#plt.suptitle('Correlation between Emotions Using Emoji-to-Emotion Projections - All Shared Emojis')\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kolmogorov-Smirnov Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "assert (weibo_projs_df.index == twitr_projs_df.index).all()\n",
    "_ = lambda emo: pd.Series(ks_2samp(weibo_projs_df[emo], twitr_projs_df[emo]), name=emo)\n",
    "ks_df = pd.concat([_(emo) for emo in weibo_projs_df.columns], axis=1).T\n",
    "ks_df.columns = ['Kolmogorov-Smirnov Statistic', 'p-Value']\n",
    "ks_df.sort_values('Kolmogorov-Smirnov Statistic', ascending=False).to_latex('Kolmogorov-Smirnov.tex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violin Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = dict(zip(ks_df.index, ks_df.index+'\\n ($D='+ks_df.iloc[:,0].round(3).astype(str)+'$)'))\n",
    "\n",
    "weibo_projs_longForm_df = weibo_projs_df.rename(columns=y_labels).stack().reset_index()\n",
    "weibo_projs_longForm_df.columns = ['emj', 'emo', 'val']\n",
    "weibo_projs_longForm_df['Platform'] = 'Weibo'\n",
    "\n",
    "twitr_projs_longForm_df = twitr_projs_df.rename(columns=y_labels).stack().reset_index()\n",
    "twitr_projs_longForm_df.columns = ['emj', 'emo', 'val']\n",
    "twitr_projs_longForm_df['Platform'] = 'Twitter'\n",
    "\n",
    "projs_longForm_df = pd.concat([weibo_projs_longForm_df, twitr_projs_longForm_df])\n",
    "\n",
    "# Plot:\n",
    "fig, ax = plt.subplots(1,1, figsize=(9, 5))#(5, 10))\n",
    "sns.violinplot(data=projs_longForm_df, ax=ax, x='emo', y='val', hue='Platform', palette=\"Set1\", split=True, scale=\"count\", bw=.2, inner=\"quartile\")#, orient='h')\n",
    "\n",
    "#plt.title('Emoji-to-Emotion Correlations')\n",
    "plt.ylabel('Emoji-Emotion Similarity')\n",
    "plt.xlabel('')#('Similarity')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
